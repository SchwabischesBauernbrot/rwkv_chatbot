{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/harrisonvanderbyl/rwkv_chatbot.git\n",
    "%cd rwkv_chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/BlinkDL/rwkv-3-pile-169m/resolve/main/RWKV-3-Pile-20220720-10704.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"MODELNAME\"]=\"https://huggingface.co/BlinkDL/rwkv-3-pile-169m/resolve/main/RWKV-3-Pile-20220720-10704.pth\"\n",
    "os.environ[\"RUN_DEVICE\"]=\"cuda\"\n",
    "os.environ[\"VRAMSTORE\"]=\"40\"\n",
    "os.environ[\"FLOAT_MODE\"]=\"bf16\"\n",
    "\n",
    "print(os.environ.get(\"MODELNAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n",
    "########################################################################################################\n",
    "\n",
    "from genericpath import exists\n",
    "from typing import List\n",
    "from src.model_run import RWKV_RNN\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "import time\n",
    "import gc\n",
    "import torch\n",
    "from src.utils import TOKENIZER\n",
    "from tqdm import tqdm\n",
    "import wget\n",
    "try:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = sys.argv[1]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "def loadModel():\n",
    "    files = os.listdir()\n",
    "    # filter by ending in .pth\n",
    "    files = [f for f in files if f.endswith(\".pth\")]\n",
    "\n",
    "    file = os.environ[\"MODELNAME\"]\n",
    "\n",
    "    url = file\n",
    "\n",
    "    file = file.split(\"/\")[-1]\n",
    "\n",
    "    if not file in os.listdir():\n",
    "        file = wget.download(url,None,wget.bar_adaptive)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    np.set_printoptions(precision=4, suppress=True, linewidth=200)\n",
    "    args = {}\n",
    "    argsnums = {}\n",
    "\n",
    "    ########################################################################################################\n",
    "    # Step 1: set model & config\n",
    "    # Do this first: pip install torchdynamo\n",
    "    ########################################################################################################\n",
    "\n",
    "    vocab_size = 50277\n",
    "\n",
    "    # 'cpu' (already very fast) // 'cuda' // proc (faster then cpu, uses a fraction of the vram of cuda)\n",
    "    args[\"RUN_DEVICE\"] = os.environ[\"RUN_DEVICE\"]\n",
    "\n",
    "    # how many layers to offload to cuda, smaller number is slower, but uses less vram. // 0 -> n_layer // use to speed up proc as well\n",
    "    numdevices = int(torch.cuda.device_count())\n",
    "    layerdist = []\n",
    "    if (args[\"RUN_DEVICE\"] == \"cuda\"):\n",
    "        for devic in range(numdevices):\n",
    "            dev = os.environ[\"VRAMSTORE\"]\n",
    "            if dev == \"\":\n",
    "                dev = 100\n",
    "            else:\n",
    "                dev = int(dev)\n",
    "            layerdist += [f\"cuda:{devic}\"] * dev\n",
    "    print(layerdist)\n",
    "\n",
    "    if (args[\"RUN_DEVICE\"] == \"cuda\"):\n",
    "       \n",
    "        layerdist += [\"proc\"] * 100 + [\"cuda:0\"]\n",
    "                                                                    \n",
    "\n",
    "    else:\n",
    "        layerdist = [\"cpu\"]*100\n",
    "    # fp32 // bf16 (saves VRAM, slightly less accurate) // fp16 (saves VRAM, slightly less accurate, can only be used with cuda, sometimes faster)\n",
    "    args[\"FLOAT_MODE\"] = os.environ[\"FLOAT_MODE\"]\n",
    "    # print config\n",
    "    print(\"RUN_DEVICE:\", args[\"RUN_DEVICE\"])\n",
    "    print(\"FLOAT_MODE:\", args[\"FLOAT_MODE\"])\n",
    "    print(\"cudalayers:\", argsnums[\"cudalayers\"]\n",
    "          if \"cudalayers\" in argsnums else \"all\")\n",
    "    print(\"\")\n",
    "\n",
    "    torch.set_num_threads(12)\n",
    "    # opt\n",
    "    opt = \"jit\"  # none // jit\n",
    "\n",
    "    if (args[\"RUN_DEVICE\"] == \"cpu\" and args[\"FLOAT_MODE\"] == \"fp16\"):\n",
    "        raise (Warning(\"fp16 is only supported on cuda\"))\n",
    "\n",
    "    args[\"MODEL_NAME\"] = file\n",
    "    argsnums[\"ctx_len\"] = 4068\n",
    "    argsnums[\"vocab_size\"] = vocab_size\n",
    "    argsnums[\"head_qk\"] = 0\n",
    "    argsnums[\"pre_ffn\"] = 0\n",
    "    argsnums[\"grad_cp\"] = 0\n",
    "    argsnums[\"my_pos_emb\"] = 0\n",
    "    os.environ[\"RWKV_RUN_DEVICE\"] = args[\"RUN_DEVICE\"]\n",
    "\n",
    "    ########################################################################################################\n",
    "    # Step 2: set prompt & sampling stuffs\n",
    "    ########################################################################################################\n",
    "    model = RWKV_RNN(args, argsnums, layerdist)\n",
    "    # if (opt == \"jit\"):\n",
    "\n",
    "    #     model = torch.jit.script(model)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:495: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "RUN_DEVICE: cuda\n",
      "FLOAT_MODE: bf16\n",
      "cudalayers: all\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "HIP error: invalid device ordinal\nHIP kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing HIP_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 61\u001b[0m\n\u001b[1;32m     57\u001b[0m DEBUG_DEBUG \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# True False --> show softmax output\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m########################################################################################################\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m model \u001b[39m=\u001b[39m loadModel()\n\u001b[1;32m     64\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOptimizing speed...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     65\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n",
      "Cell \u001b[0;32mIn [27], line 104\u001b[0m, in \u001b[0;36mloadModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mRWKV_RUN_DEVICE\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m args[\u001b[39m\"\u001b[39m\u001b[39mRUN_DEVICE\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    101\u001b[0m \u001b[39m########################################################################################################\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39m# Step 2: set prompt & sampling stuffs\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39m########################################################################################################\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m model \u001b[39m=\u001b[39m RWKV_RNN(args, argsnums, layerdist)\n\u001b[1;32m    105\u001b[0m \u001b[39m# if (opt == \"jit\"):\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[39m#     model = torch.jit.script(model)\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/projects/rwkv-chat/src/model_run.py:121\u001b[0m, in \u001b[0;36mRWKV_RNN.__init__\u001b[0;34m(self, args, argsnumns, layers)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m (args[\u001b[39m\"\u001b[39m\u001b[39mRUN_DEVICE\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m (w[x]\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 121\u001b[0m         w[x] \u001b[39m=\u001b[39m w[x]\u001b[39m.\u001b[39;49mpin_memory()\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mblocks.\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m x) \u001b[39mor\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mblocks.0.\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m x):\n\u001b[1;32m    123\u001b[0m     \u001b[39mif\u001b[39;00m print_need_newline:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: HIP error: invalid device ordinal\nHIP kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing HIP_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "########################################################################################################\n",
    "# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n",
    "########################################################################################################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import torch\n",
    "from src.utils import TOKENIZER\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = sys.argv[1]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "########################################################################################################\n",
    "# Step 2: set prompt & sampling stuffs\n",
    "########################################################################################################\n",
    "\n",
    "# context = 'A'\n",
    "# context = \"\\nIn the\"\n",
    "# context = '\\nSugar:'\n",
    "\n",
    "# context = \"\\n深圳是\" # test Chinese\n",
    "# context = \"\\n東京は\" # test Japanese\n",
    "\n",
    "###### A good prompt for chatbot ######\n",
    "context = '''\n",
    "The following is a conversation between a highly knowledgeable and intelligent AI assistant, called RWKV, and a human user, called User. In the following interactions, User and RWKV will converse in natural language, and RWKV will do its best to answer User’s questions. RWKV was built to be respectful, polite and inclusive. It knows a lot, and always tells the truth. The conversation begins.\n",
    "\n",
    "User: OK RWKV, I’m going to start by quizzing you with a few warm-up questions. Who is currently the president of the USA?\n",
    "\n",
    "RWKV: It’s Joe Biden; he was sworn in earlier this year.\n",
    "\n",
    "User: What year was the French Revolution?\n",
    "\n",
    "RWKV: It started in 1789, but it lasted 10 years until 1799.\n",
    "\n",
    "User: Can you guess who I might want to marry?\n",
    "\n",
    "RWKV: Only if you tell me more about yourself - what are your interests?\n",
    "\n",
    "User: Aha, I’m going to refrain from that for now. Now for a science question. What can you tell me about the Large Hadron Collider (LHC)?\n",
    "\n",
    "RWKV: It’s a large and very expensive piece of science equipment. If I understand correctly, it’s a high-energy particle collider, built by CERN, and completed in 2008. They used it to confirm the existence of the Higgs boson in 2012.\n",
    "\n",
    "'''\n",
    "# context = \"hello world! I am your supreme overlord!\"\n",
    "NUM_TRIALS = 999\n",
    "LENGTH_PER_TRIAL = 200\n",
    "\n",
    "TEMPERATURE = 1.0\n",
    "top_p = 0.9\n",
    "top_p_newline = 0.9  # only used in TOKEN_MODE = char\n",
    "\n",
    "DEBUG_DEBUG = False  # True False --> show softmax output\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "model = loadModel()\n",
    "\n",
    "\n",
    "print(f'\\nOptimizing speed...')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# input(0)\n",
    "TOKEN_MODE = \"pile\"\n",
    "WORD_NAME = [\n",
    "    \"20B_tokenizer.json\",\n",
    "    \"20B_tokenizer.json\",\n",
    "]  # [vocab, vocab] for Pile model\n",
    "UNKNOWN_CHAR = None\n",
    "print(f'\\nLoading tokenizer {WORD_NAME}...')\n",
    "tokenizer = TOKENIZER(WORD_NAME, UNKNOWN_CHAR=UNKNOWN_CHAR)\n",
    "if TOKEN_MODE == \"pile\":\n",
    "    assert tokenizer.tokenizer.decode([187]) == '\\n'\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Note: currently the first run takes a while if your prompt is long, as we are using RNN to preprocess the prompt. Use GPT to build the hidden state for better speed.\\n\"\n",
    ")\n",
    "\n",
    "time_slot = {}\n",
    "time_ref = time.time_ns()\n",
    "\n",
    "\n",
    "def record_time(name):\n",
    "    if name not in time_slot:\n",
    "        time_slot[name] = 1e20\n",
    "    tt = (time.time_ns() - time_ref) / 1e9\n",
    "    if tt < time_slot[name]:\n",
    "        time_slot[name] = tt\n",
    "\n",
    "\n",
    "init_out = []\n",
    "\n",
    "out = []\n",
    "\n",
    "print(\"torch.cuda.memory_allocated: %fGB\" %\n",
    "      (torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\" %\n",
    "      (torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\" %\n",
    "      (torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "\n",
    "# bot.py\n",
    "\n",
    "# init empty save state and question context\n",
    "model_tokens = tokenizer.tokenizer.encode(context)\n",
    "print(tokenizer.tokenizer.encode(\"\\n\\n\"))\n",
    "#  see if save_state file exists\n",
    "if os.path.isfile(f\"save_states_{model.n_emb}_{model.n_layer}.pt\"):\n",
    "    print(\"Loading save state...\")\n",
    "    savestates = torch.load(f\"save_states_{model.n_emb}_{model.n_layer}.pt\")\n",
    "    state = savestates[\"init\"]\n",
    "\n",
    "else:\n",
    "    state = model.loadContext(newctx=model_tokens)\n",
    "    savestates = {\n",
    "        \"init\": (state[0], state[1].clone())\n",
    "    }\n",
    "    torch.save(savestates, f\"save_states_{model.n_emb}_{model.n_layer}.pt\")\n",
    "\n",
    "for TRIAL in range(1 if DEBUG_DEBUG else NUM_TRIALS):\n",
    "    print(\"--\")\n",
    "    time_ref = time.time_ns()\n",
    "    inp = input('User: ')\n",
    "    if (inp.startswith(\"save \")):\n",
    "        savestates[inp[5:]] = (state[0], state[1].clone())\n",
    "        # Save to file\n",
    "        torch.save(savestates, f\"save_states_{model.n_emb}_{model.n_layer}.pt\")\n",
    "        print(\"Saved to file.\")\n",
    "        continue\n",
    "    if (inp.startswith(\"load \")):\n",
    "\n",
    "        savestates = torch.load(\n",
    "            f\"save_states_{model.n_emb}_{model.n_layer}.pt\")\n",
    "        state = (savestates[inp[5:]][0], savestates[inp[5:]][1].clone())\n",
    "\n",
    "        continue\n",
    "    state = model.loadContext(ctx=state[0], statex=state[1], newctx=tokenizer.tokenizer.encode(\n",
    "        f\"\\n\\nUser: {inp}\\n\\nRWKV:\"))\n",
    "\n",
    "    state = [{\"score\": 1, \"state\": state[1], \"ctx\": state[0]}]\n",
    "\n",
    "    if TRIAL == 0:\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(100):\n",
    "\n",
    "            state = model.run(\n",
    "                state, temp=TEMPERATURE, top_p=top_p)\n",
    "            print(tokenizer.tokenizer.decode(state[0][\"ctx\"]))\n",
    "    state = (state[0][\"ctx\"], state[0][\"state\"])\n",
    "    print(tokenizer.tokenizer.decode(state[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
